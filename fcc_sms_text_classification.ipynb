{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*\n",
    "\n",
    "---\n",
    "\n",
    "In this challenge, you need to create a machine learning model that will classify SMS messages as either \"ham\" or \"spam\". A \"ham\" message is a normal message sent by a friend. A \"spam\" message is an advertisement or a message sent by a company.\n",
    "\n",
    "You should create a function called `predict_message` that takes a message string as an argument and returns a list. The first element in the list should be a number between zero and one that indicates the likeliness of \"ham\" (0) or \"spam\" (1). The second element in the list should be the word \"ham\" or \"spam\", depending on which is most likely.\n",
    "\n",
    "For this challenge, you will use the [SMS Spam Collection dataset](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/). The dataset has already been grouped into train data and test data.\n",
    "\n",
    "The first two cells import the libraries and data. The final cell tests your model and function. Add your code in between these cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8RZOuS9LWQvv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "googleapis-common-protos 1.56.2 requires protobuf<4.0.0dev,>=3.15.0, but you'll have protobuf 3.13.0 which is incompatible.\n",
      "c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow-datasets) (0.10.0)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow-datasets) (1.21.5)\n",
      "Requirement already satisfied: six in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow-datasets) (2.24.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.8.0-py3-none-any.whl (50 kB)\n",
      "Collecting etils[epath]\n",
      "  Downloading etils-0.6.0-py3-none-any.whl (98 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.8.0-py3-none-any.whl (28 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow-datasets) (3.13.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tqdm->tensorflow-datasets) (0.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.56.2-py2.py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: zipp; extra == \"epath\" in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from etils[epath]->tensorflow-datasets) (3.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\husba\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from protobuf>=3.12.2->tensorflow-datasets) (49.6.0.post20200814)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21500 sha256=120f039128ebf315ffa3638ebad029dc9498eab67535bb17ca21599a48f24d4b\n",
      "  Stored in directory: c:\\users\\husba\\appdata\\local\\pip\\cache\\wheels\\29\\93\\c6\\762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built promise\n",
      "Installing collected packages: promise, typing-extensions, toml, tqdm, googleapis-common-protos, tensorflow-metadata, importlib-resources, etils, dill, tensorflow-datasets\n",
      "Successfully installed dill-0.3.5.1 etils-0.6.0 googleapis-common-protos-1.56.2 importlib-resources-5.8.0 promise-2.3 tensorflow-datasets-4.6.0 tensorflow-metadata-1.8.0 toml-0.10.2 tqdm-4.64.0 typing-extensions-4.2.0\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "!pip install tensorflow-datasets\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lMHwYXHXCar3"
   },
   "outputs": [],
   "source": [
    "# get data files\n",
    "#!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
    "#!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
    "\n",
    "train_file_path = \"train-data.tsv\"\n",
    "test_file_path = \"valid-data.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g_h508FEClxO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>ham</td>\n",
       "      <td>true dear..i sat to pray evening and felt so.s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>ham</td>\n",
       "      <td>what will we do in the shower, baby?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>ham</td>\n",
       "      <td>where are you ? what are you doing ? are yuou ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>spam</td>\n",
       "      <td>ur cash-balance is currently 500 pounds - to m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>spam</td>\n",
       "      <td>not heard from u4 a while. call 4 rude chat pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cc                                               text  code\n",
       "1387   ham  true dear..i sat to pray evening and felt so.s...     0\n",
       "1388   ham               what will we do in the shower, baby?     0\n",
       "1389   ham  where are you ? what are you doing ? are yuou ...     0\n",
       "1390  spam  ur cash-balance is currently 500 pounds - to m...     1\n",
       "1391  spam  not heard from u4 a while. call 4 rude chat pr...     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.read_csv(train_file_path,sep=\"\\t\",header=None)\n",
    "test_data=pd.read_csv(test_file_path,sep=\"\\t\",header=None)\n",
    "\n",
    "train_data.columns = ['cc', 'text' ]\n",
    "test_data.columns = ['cc', 'text' ]\n",
    "\n",
    "\n",
    "train_data.cc = pd.Categorical(train_data.cc)\n",
    "test_data.cc = pd.Categorical(test_data.cc)\n",
    "train_data['code'] = train_data.cc.cat.codes\n",
    "test_data['code'] = test_data.cc.cat.codes\n",
    "\n",
    "train_data.tail()\n",
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zOMKywn4zReN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 100) (1392, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "y_train=train_data['code']\n",
    "y_test=test_data['code']\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0GjX9N-itomo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           246176    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                102432    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 348,641\n",
      "Trainable params: 348,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 32\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1 , \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fYB8KiyYtrAu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "418/418 [==============================] - 2s 4ms/step - loss: 0.1851 - accuracy: 0.9349 - val_loss: 0.0665 - val_accuracy: 0.9784\n",
      "Epoch 2/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9885 - val_loss: 0.0576 - val_accuracy: 0.9806\n",
      "Epoch 3/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0200 - accuracy: 0.9952 - val_loss: 0.0473 - val_accuracy: 0.9835\n",
      "Epoch 4/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0117 - accuracy: 0.9971 - val_loss: 0.0483 - val_accuracy: 0.9856\n",
      "Epoch 5/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0506 - val_accuracy: 0.9864\n",
      "Epoch 6/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 0.0651 - val_accuracy: 0.9835\n",
      "Epoch 7/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0618 - val_accuracy: 0.9842\n",
      "Epoch 8/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.0577 - val_accuracy: 0.9856\n",
      "Epoch 9/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0594 - val_accuracy: 0.9856\n",
      "Epoch 10/10\n",
      "418/418 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0609 - val_accuracy: 0.9856\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "J9tD9yACG6M9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9469135e-06]]\n",
      "[array([[1.9469135e-06]], dtype=float32), 'ham']\n"
     ]
    }
   ],
   "source": [
    "# function to predict messages based on model\n",
    "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
    "def predict_message(pred_text):  \n",
    "    pred_list=[pred_text]\n",
    "    pred_data = tokenizer.texts_to_sequences(pred_list)\n",
    "    pred_data = pad_sequences(pred_data, padding='post', maxlen=maxlen)\n",
    "    pred_data=np.array(pred_data)\n",
    "    # get the prediction\n",
    "    predict= model.predict(pred_data)\n",
    "    print(predict)\n",
    "    if predict >= 0.5 :\n",
    "        prediction=[predict,'spam']\n",
    "    else:\n",
    "        prediction=[predict,'ham']\n",
    "    return (prediction)\n",
    "pred_text = \"how are you doing today?\"\n",
    "\n",
    "prediction = predict_message(pred_text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Dxotov85SjsC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9469135e-06]]\n",
      "[[0.6815942]]\n",
      "[[5.758235e-08]]\n",
      "[[0.9994186]]\n",
      "[[0.99959046]]\n",
      "[[9.657623e-08]]\n",
      "[[2.2246842e-07]]\n",
      "You passed the challenge. Great job!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function and model. Do not modify contents.\n",
    "def test_predictions():\n",
    "  test_messages = [\"how are you doing today\",\n",
    "                   \"sale today! to stop texts call 98912460324\",\n",
    "                   \"i dont want to go. can we try it a different day? available sat\",\n",
    "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
    "                   \"you have won Â£1000 cash! call to claim your prize.\",\n",
    "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
    "                   \"wow, is your arm alright. that happened to me one time too\"\n",
    "                  ]\n",
    "\n",
    "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
    "  passed = True\n",
    "\n",
    "  for msg, ans in zip(test_messages, test_answers):\n",
    "    prediction = predict_message(msg)\n",
    "    if prediction[1] != ans:\n",
    "      passed = False\n",
    "\n",
    "  if passed:\n",
    "    print(\"You passed the challenge. Great job!\")\n",
    "  else:\n",
    "    print(\"You haven't passed yet. Keep trying.\")\n",
    "\n",
    "test_predictions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fcc_sms_text_classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
